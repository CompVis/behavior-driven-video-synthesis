<!DOCTYPE HTML>
<!--
  Based on
	Spatial by TEMPLATED
	templated.co @templatedco
	Released for free under the Creative Commons Attribution 3.0 license (templated.co/license)
-->
<html>
	<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117339330-4"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-117339330-4');
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <title>
      Behavior-Driven Synthesis of Human Dynamics
    </title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="landing">

		<!-- Banner -->
			<section id="banner" style="background-attachment:scroll;">
        <h2>
          Behavior-Driven Synthesis of Human Dynamics
        </h2>
        <p>
        <a href="https://www.linkedin.com/in/andreas-blattmann-479038186/?originalSubdomain=de">Andreas Blattmann</a>&ast;, 
        <a href="hhttps://timomilbich.github.io/">Timo Milbich</a>&ast;,
        <a href="https://mdork.github.io/">Michael Dorkenwald</a>&ast;,
        <a href="https://hci.iwr.uni-heidelberg.de/Staff/bommer">Bj&ouml;rn Ommer</a><br/>
        <a href="https://www.iwr.uni-heidelberg.de/">HCI/IWR, Heidelberg University</a><br/>
        <a href="http://cvpr2021.thecvf.com/">CVPR 2021</a><br/>
        </p>
			</section>

<!--			&lt;!&ndash; One &ndash;&gt;-->
<!--				<section id="one" class="wrapper style1">-->
<!--					<div class="container 75%">-->

<!--						<div class="row 200%">-->
<!--							<div class="6u 12u$(medium) vert-center" style="margin:1% 0">-->
<!--                  <div class="container 25%">-->


<!--                    <div class="image fit captioned align-center"-->
<!--                                style="margin-bottom:0em; box-shadow:0 0">-->
<!--                      <a href="paper/paper.pdf">-->
<!--                        <img src="paper/paper.png" alt="" style="border:1px solid black"/>-->
<!--                      </a>-->
<!--                      <a href="https://arxiv.org/abs/2103.04677">arXiv</a>-->
<!--                      <div class="headerDivider"></div>-->
<!--                      <a href="paper/paper.bib">BibTeX</a>-->
<!--                      <div class="headerDivider"></div>-->
<!--                      <a href="https://github.com/CompVis/behavior-driven-video-synthesis">GitHub</a>-->
<!--                      <br/>-->
<!--                      &ast; indicates equal contribution-->
<!--                    </div>-->

<!--                  </div>-->
<!--							</div>-->
<!--							<div class="6u$ 12u$(medium)">-->
<!--                <h1>Abstract</h1>-->
<!--                <p style="text-align: justify">-->
<!--                Generating and representing human behavior are of major importance for various computer vision applications. Commonly, human video synthesis represents behavior as sequences of postures while directly predicting their likely progressions or merely changing the appearance of the depicted persons, thus not being able to exercise control over their actual behavior during the synthesis process. In contrast, controlled behavior synthesis and transfer across individuals requires a deep understanding of body dynamics and calls for a representation of behavior that is independent of appearance and also of specific postures. In this work, we present a model for human behavior synthesis which learns a dedicated representation of human dynamics independent of postures. Using this representation, we are able to change the behavior of a person depicted in an arbitrary posture, or to even directly transfer behavior observed in a given video sequence. To this end, we propose a conditional variational framework which explicitly disentangles posture from behavior. We demonstrate the effectiveness of our approach on this novel task, evaluating capturing, transferring, and sampling fine-grained, diverse behavior, both quantitatively and qualitatively.-->
<!--                </p>-->

<!--							</div>-->
<!--						</div>-->

<!--				</section>-->

        <!-- One -->
				<section id="one" class="wrapper style1">
                    <div class="container 80%">
                    <div class="image fit captioned align-left"
                                style="margin-bottom:2em; box-shadow:0 0;
                                text-align:justify">
                      <img src="images/first-page.png" alt="" style="border:0px solid black"/>
                      <span style="font-weight: bold">TL;DR:</span> Our Approach for Behavior Transfer. Given a source sequence of human dynamics our model infers a behavior encoding which is independent of posture. We can re-enact the behavior by combining it with an unrelated target posture and thus control the synthesis process. The resulting sequence is combined with an appearance to synthesize a video sequence
                    </div>
                    </div>
					<div class="container 100%">

						<div class="row 200%">
							<div class="6u 12u$(medium) vert-center" style="margin:1% 0">
                  <div class="container 25%">

                    <div class="image fit captioned align-center"
                                style="margin-bottom:0em; box-shadow:0 0">
                      <a href="paper/paper.pdf">
                        <img src="paper/paper.png" alt="" style="border:1px solid black"/>
                      </a>
                      <a href="https://github.com/CompVis/interactive-image2video-synthesis">GitHub</a>
                    </div>

                  </div>
							</div>
							<div class="6u$ 12u$(medium)">
                <h1>Abstract</h1>
                <p style="text-align: justify">
                Generating and representing human behavior are of major importance for various computer vision applications. Commonly, human video synthesis represents behavior as sequences of postures while directly predicting their likely progressions or merely changing the appearance of the depicted persons, thus not being able to exercise control over their actual behavior during the synthesis process. In contrast, controlled behavior synthesis and transfer across individuals requires a deep understanding of body dynamics and calls for a representation of behavior that is independent of appearance and also of specific postures. In this work, we present a model for human behavior synthesis which learns a dedicated representation of human dynamics independent of postures. Using this representation, we are able to change the behavior of a person depicted in an arbitrary posture, or to even directly transfer behavior observed in a given video sequence. To this end, we propose a conditional variational framework which explicitly disentangles posture from behavior. We demonstrate the effectiveness of our approach on this novel task, evaluating capturing, transferring, and sampling fine-grained, diverse behavior, both quantitatively and qualitatively.
                </p>

							</div>
						</div>
                    </div>


				</section>


<!--			&lt;!&ndash; Two &ndash;&gt;-->
<!--				<section id="two" class="wrapper style2 special">-->
<!--					<div class="container">-->
<!--						<header class="major">-->
<!--							<h2>Approach</h2>-->
<!--						</header>-->

<!--            <div class="row 150%">-->
<!--            	</section>-->

<!--			&lt;!&ndash; One &ndash;&gt;-->
<!--				<section id="one" class="wrapper style1">-->
<!--					<div class="container 75%">-->
<!--												<div class="image fit captioned align-left"-->
<!--                                style="margin-bottom:2em; box-shadow:0 0;-->
<!--                                text-align:justify">-->
<!--                      <img src="images/first-page.png" alt="" style="border:0px solid black"/>-->
<!--                      Our Approach for Behavior Transfer. Given a source sequence of human dynamics our model infers a behavior encoding which is independent of posture. We can re-enact the behavior by combining it with an unrelated target posture and thus control the synthesis process. The resulting sequence is combined with an appearance to synthesize a video sequence.-->
<!--                    </div>-->
<!--                    <div class="image fit captioned align-left"-->
<!--                                style="margin-bottom:2em; box-shadow:0 0;-->
<!--                                text-align:justify">-->
<!--                      <img src="paper/teaser.png" alt="" style="border:0px solid black"/>-->
<!--                      During training we learn an behavior representation using a special conditional VAE which is independent of posture. Therefore, we use an auxiliary decoder to disentangle behavior from posture. -->
<!--                      In inference, we transfer source behavior (green) to an arbitrary target posture (yellow) or synthesize novel behavior from the prior distribution which is-->
<!--						matched to q by a learned invertible transformation T (red).-->
<!--                    </div>-->
<!--            <div class="row 150%">-->
<!--            	</section>-->
        <!-- Two -->
				<section id="two" class="wrapper style2 special">
					<div class="container">
						<header class="major">
							<h2>Approach</h2>
						</header>

                        <div class="container 80%">
                    <div class="image fit captioned align-left"
                                style="margin-bottom:2em; box-shadow:0 0;
                                text-align:justify">
                      <img src="paper/teaser.png" alt="" style="border:0px solid black"/>
                      During training we learn an behavior representation \(z_\beta\) which is independent of posture by combining a special conditional VAE with an auxiliary decoder \(p_\psi\) measuring the amount of posture present in the behavior sequence. To improve synthesis quality, we relax the latent regularization term in the cVAE-objective and train an invertible transformation \(\mathcal{T}_\xi\) to close the resulting gap between the prior and posterior distributions in a second stage.
                      In inference mode, we transfer a given source behavior (green) to an arbitrary target posture (yellow) or synthesize novel behavior from the prior distribution which is matched to q by a learned invertible transformation T (red).
                    </div>
                        </div>
                    </div>
                </section>


        <section id="three" class="wrapper style2 special">
          <div class="container">
            <header class="major">
              <h2>Results</h2>
              <p>and applications of our model.</p>
            </header>


              <header class="minor">
                <h2>Behavior Transfer</h2>
              </header>
              <p>
                   We transfer fine-grained, characteristic body dynamics of an observed behavior \(x_\beta\) to unrelated, significantly different target postures \(x_t\). If required, the target posture is first adjusted by a transition phase before re-enacting the inferred behavior.
              </p>

               <div class="row 150%">
                <div class="6u 12u$(xsmall)">
                    <div class="image fit captioned align-just">
                        <div class="videocontainer">
                            <video controls class="videothing">
                            <source src="images/behavior_transfer1.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                            </video>
                        </div>
                        Visualization of transferred behavior in posture space: Each column shows distinct re-enactements of a given source behavior \(\boldsymbol{x}_\beta\), visualized in the top-row example, for multiple target postures \(x_t\).
                    </div>
                </div>
                <div class="6u$ 12u$(xsmall)">

                    <div class="image fit captioned align-just">
                    <div class="videocontainer">
                    <video controls class="videothing">
                    <source src="images/behavior_transfer1_RGB.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                    </video>
                    </div>
                    Visualization of transferred behavior in RGB-space: We generate video sequences by first generating re-enacted posture sequences as explained above. As our proposed disentangling framework can also be applied to posture-appearance transfer (see below for more examples), we can subsequently train a model to disentangle posture from appearance and apply it to generate the individual image frames of a re-enacted posture sequence for a given human appearance. The above examples visualize this ability based on the synthesized posture sequences shown on the left. Note the difference in appearance between the source sequence and the resulting video re-enactments.
                    </div>
                </div>
            </div>
            <div class="row 150%">
                <div class="6u 12u$(xsmall)">
                    <div class="image fit captioned align-just">
                        <div class="videocontainer">
                            <video controls class="videothing">
                            <source src="images/behavior_transfer2.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                            </video>
                        </div>
                        Visualization of transferred behavior in posture space: Each column shows distinct re-enactements of a given source behavior \(\boldsymbol{x}_\beta\), visualized in the top-row example, for multiple target postures \(x_t\).
                    </div>
                </div>
                <div class="6u$ 12u$(xsmall)">

                    <div class="image fit captioned align-just">
                    <div class="videocontainer">
                    <video controls class="videothing">
                    <source src="images/behavior_transfer2_RGB.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                    </video>
                    </div>
                    Visualization of transferred behavior in RGB-space: We generate video sequences by first generating re-enacted posture sequences as explained above. As our proposed disentangling framework can also be applied to posture-appearance transfer (see below for more examples), we can subsequently train a model to disentangle posture from appearance and apply it to generate the individual image frames of a re-enacted posture sequence for a given human appearance. The above examples visualize this ability based on the synthesized posture sequences shown on the left. Note the difference in appearance between the source sequence and the resulting video re-enactments.
                    </div>
                </div>
            </div>

        <header class="minor">
            <h2>Behavior Sampling</h2>
          </header>
          <p>
               As we learn a parametric prior behavior distribution \(p(z_\beta)\), we can use our model to sample novel, unseen behaviors for a given target posture.
          </p>


        <div class="row 150%">
            <div class="6u 12u$(xsmall)">
                <div class="image fit captioned align-just">
                    <div class="videocontainer">
                        <video controls class="videothing">
                        <source src="images/samples.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                        </video>
                    </div>
                    To visually evaluate the diversity of behavior sequences sampled by our model we visualize six samples drawn from \(z_\beta\) for the same target posture in each row.
                </div>
            </div>
            <div class="6u$ 12u$(xsmall)">

                <div class="image fit captioned align-just">
                <div class="videocontainer">
                <video controls class="videothing">
                <source src="images/nearest_neighbors.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                </div>
                Nearest neighour visualization in behavior and posture space: We re-enact a source behavior \(\boldsymbol{x}_\beta\) using a random target posture \(x_t\). Next, we find its nearest neighbour in the training sequences based on <i>(i)</i> distance between behavior representations \(z_\beta\) and <i>(ii)</i> average distances between postures sequences (based on alignment w.r.t. the pelvis keypoints). Each column depicts a separate example showing the 'Source Behavior', the 'Nearest Neighbor based on Behavior representation', the 'Behavior Re-enactment of Source Behavior' and the 'Nearest Neighbor based on Posture', i.e. average posture distance. We observe that while gthere exist close training sequences in terms of posture, the nearest neighbors based on \(z_\beta\) show <i>similar</i> behavior dynamics while being <i>dissimilar</i> in posture.
                </div>
            </div>
        </div>

        <header class="minor">
            <h2>Behavior Interpolation</h2>
          </header>
          <p>
               We interpolate between the behavior observed in two sequences \(\boldsymbol{x}_\beta^1\) and \(\boldsymbol{x}_\beta^2\). To this end, we first extract their corresponding behavior representations \(z_\beta^1, z_\beta^2\) and interpolate between them at equidistant steps, i.e. \((1 - \lambda) \cdot z_\beta^1 + \lambda \cdot z_\beta^2; \; \lambda \in \{0.0,0.2,0.4,0.6,0.8,1.0\}\). Next, we generate a sequence of interpolated behavior using our decoder \(p_\theta(\boldsymbol{x}|z_\beta,x_t)\) with \(x_t\) being the first frame of \(\boldsymbol{x}_\beta^1\), respectively \(\boldsymbol{x}_\beta^2\). Note, that for \(\lambda \in \{0,1.0\}\) we basically reconstruct the source sequences \(\boldsymbol{x}_\beta^1\), \(\boldsymbol{x}_\beta^2\).
          </p>

        <div class="row 150%">
            <div class="6u 12u$(xsmall)">
                <div class="image fit captioned align-just">
                    <div class="videocontainer">
                        <video controls class="videothing">
                        <source src="images/interpolations_01.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                        </video>
                    </div>
                    Interpolated behavior sequences between two sequences \(\boldsymbol{x}_\beta^1\) and \(\boldsymbol{x}_\beta^2\).
                </div>
            </div>
            <div class="6u$ 12u$(xsmall)">

                <div class="image fit captioned align-just">
                <div class="videocontainer">
                <video controls class="videothing">
                <source src="images/interpolations_rgb_01.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video>
                </div>
                Video sequences synthesized by our model for the interpolated behavior sequences shown on the left.
                </div>
            </div>
        </div>
        <div class="row 150%">
            <div class="6u 12u$(xsmall)">
                <div class="image fit captioned align-just">
                    <div class="videocontainer">
                        <video controls class="videothing">
                        <source src="images/interpolations_02.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                        </video>
                    </div>
                    Interpolated behavior sequences between two sequences \(\boldsymbol{x}_\beta^1\) and \(\boldsymbol{x}_\beta^2\).
                </div>
            </div>
            <div class="6u$ 12u$(xsmall)">

                <div class="image fit captioned align-just">
                <div class="videocontainer">
                <video controls class="videothing">
                <source src="images/interpolations_rgb_02.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video>
                </div>
                Video sequences synthesized by our model for the interpolated behavior sequences shown on the left.
                </div>
            </div>
        </div>
        <div class="row 150%">
            <div class="6u 12u$(xsmall)">
                <div class="image fit captioned align-just">
                    <div class="videocontainer">
                        <video controls class="videothing">
                        <source src="images/interpolations_03.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                        </video>
                    </div>
                    Interpolated behavior sequences between two sequences \(\boldsymbol{x}_\beta^1\) and \(\boldsymbol{x}_\beta^2\).
                </div>
            </div>
            <div class="6u$ 12u$(xsmall)">

                <div class="image fit captioned align-just">
                <div class="videocontainer">
                <video controls class="videothing">
                <source src="images/interpolations_rgb_03.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video>
                </div>
                Video sequences synthesized by our model for the interpolated behavior sequences shown on the left.
                </div>
            </div>
        </div>

    <header class="minor">
            <h2>Posture-Appearance Transfer</h2>
          </header>
          <p>
               Our approach to disentangling latent representations is not only appicable for factorizing static from temporal information, but also to posture-appearance disentangling. We demonstrate this capability of our model on the DeepFashion and Market1501 datasets.
          </p>

    <div class="row 150%">
        <div class="6u 12u$(xsmall)">

            <div class="image fit captioned align-just">
                <a href="images/deepfashion_grid.jpg">
                <img src="images/deepfashion_grid.jpg" alt="" />
                </a>
                Posture-Appearance transfer on DeepFashion. Our method can also be used to disentangle posture and appearance. A quantitative evaluation can be found Table 1 in the supplementary material.
            </div>
            </div>
        <div class="6u$ 12u$(xsmall)">

            <div class="image fit captioned align-just">
                <a href="images/market_grid.jpg">
                <img src="images/market_grid.jpg" alt="" />
                </a>
                Posture-Appearance transfer on Market1501. Our method can also be used to disentangle posture and appearance. A quantitative evaluation can be found Table 1 in the supplementary material.

            </div>
        </div>
    </div>


    </div>
</section>


			<!-- last -->
<section id="four" class="wrapper style3 special"
          style="background-attachment:scroll;background-position:center bottom;">
					<div class="container">
						<header class="major">
							<h2>Acknowledgement</h2>
              <p>
              	The research leading to these results is funded by the German Federal Ministry for Economic Affairs and Energy within the project “KI-Absicherung – Safe AI for automated driving” and by the German Research Foundation (DFG) within project 421703927.
              This page is based on a design by <a href="http://templated.co">TEMPLATED</a>.
              </p>
						</header>
					</div>
				</section>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
