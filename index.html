<!DOCTYPE HTML>
<!--
  Based on
	Spatial by TEMPLATED
	templated.co @templatedco
	Released for free under the Creative Commons Attribution 3.0 license (templated.co/license)
-->
<html>
	<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117339330-4"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-117339330-4');
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <title>
      Behavior-Driven Synthesis of Human Dynamics
    </title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="landing">

		<!-- Banner -->
			<section id="banner" style="background-attachment:scroll;">
        <h2>
          Behavior-Driven Synthesis of Human Dynamics
        </h2>
        <p>
        <a href="https://www.linkedin.com/in/andreas-blattmann-479038186/?originalSubdomain=de">Andreas Blattmann</a>&ast;, 
        <a href="hhttps://timomilbich.github.io/">Timo Milbich</a>&ast;,
        <a href="https://mdork.github.io/">Michael Dorkenwald</a>&ast;,
        <a href="https://hci.iwr.uni-heidelberg.de/Staff/bommer">Bj&ouml;rn Ommer</a><br/>
        <a href="https://www.iwr.uni-heidelberg.de/">HCI/IWR, Heidelberg University</a><br/>
        <a href="http://cvpr2021.thecvf.com/">CVPR 2021</a><br/>
        </p>
			</section>

        <!-- One -->
        <section id="one" class="wrapper style1">
            <div class="container 100%">
                <div class="8u" style="margin-left: 12em">
                <div class="image fit captioned align-left"
                            style="margin-bottom:2em; box-shadow:0 0;
                            text-align:left">
                  <img src="images/first-page.png" alt="" style="border:0px solid black"/>
                  <span style="font-weight: bold">TL;DR:</span> Our Approach for Behavior Transfer. Given a source sequence of human dynamics our model infers a behavior encoding which is independent of posture. We can re-enact the behavior by combining it with an unrelated target posture and thus control the synthesis process. The resulting sequence is combined with an appearance to synthesize a video sequence
                </div>
<!--                </div>-->
<!--                    </div>-->
<!--					<div class="container 100%">-->

                <div class="row 200%">
                    <div class="3u 12u$(medium) vert-center" style="margin:1% 0">
<!--                        <div class="container 50%">-->

                            <div class="image fit captioned align-center" style="margin-bottom:0em; box-shadow:0 0">
                                <a href="paper/paper.pdf">
                                    <img src="paper/paper.png" alt="" style="border:1px solid black"/>
                                </a>
                                <a href="https://github.com/CompVis/behavior-driven-video-synthesis">GitHub</a>
                            </div>

<!--                        </div>-->
                    </div>
                    <div class="9u$ 12u$(medium)">
                        <h1>Abstract</h1>
                        <p style="text-align: justify">
                            Generating and representing human behavior are of major importance for various computer vision applications. Commonly, human video synthesis
                            represents behavior as sequences of postures while directly predicting their likely progressions or merely changing the appearance of the
                            depicted persons, thus not being able to exercise control over their actual behavior during the synthesis process. In contrast, controlled
                            behavior synthesis and transfer across individuals requires a deep understanding of body dynamics and calls for a representation of behavior
                            that is independent of appearance and also of specific postures. In this work, we present a model for human behavior synthesis which learns a
                            dedicated representation of human dynamics independent of postures. Using this representation, we are able to change the behavior of a person
                            depicted in an arbitrary posture, or to even directly transfer behavior observed in a given video sequence. To this end, we propose a
                            conditional variational framework which explicitly disentangles posture from behavior. We demonstrate the effectiveness of our approach on this
                            novel task, evaluating capturing, transferring, and sampling fine-grained, diverse behavior, both quantitatively and qualitatively.
                        </p>
                    </div>
                </div>
            </div>
<!--            <div class="container">-->
<!--                <div class="image fit captioned align-left" style="margin-bottom:2em; box-shadow:0 0;-->
<!--                                 text-align:justify">-->
<!--                    <div class="row 200%">-->


<!--                    <div class="12u">-->
<!--                      <h4>Related Work on Video Synthesis</h4>-->
<!--                    </div>-->

<!--                    <div class="12u">-->
<!--                      <h6>-->
<!--                        <a href="https://compvis.github.io/interactive-image2video-synthesis/">-->
<!--                          Understanding Object Dynamics for Interactive Image-to-Video Synthesis-->
<!--                        </a>-->
<!--                      </h6>-->
<!--                    </div>-->

<!--                    <div class="3u 12u$(medium)">-->
<!--                      <div class="image fit align-center">-->
<!--                        <a href="https://compvis.github.io/interactive-image2video-synthesis/">-->
<!--                          <img src="https://compvis.github.io/interactive-image2video-synthesis/images/overview.png" style="max-width:25em; margin:auto" />-->
<!--                        </a>-->
<!--                      </div>-->
<!--                    </div>-->
<!--                    <div class="8u 12u$(medium)">-->
<!--                      <p align="justify" style="line-height: 1.0em; font-size:0.8em">-->
<!--                      What would be the effect of locally poking a static scene? We present an approach that learns naturally-looking global articulations-->
<!--                      caused by a local manipulation at a pixel level. Training requires only videos of moving objects but no information of the underlying-->
<!--                      manipulation of the physical scene. Our generative model learns to infer natural object dynamics as a response to user interaction-->
<!--                      and learns about the interrelations between different object body regions. Given a static image of an object and a local poking of a-->
<!--                      pixel, the approach then predicts how the object would deform over time. In contrast to existing work on video prediction, we do not-->
<!--                      synthesize arbitrary realistic videos but enable local interactive control of the deformation. Our model is not restricted to-->
<!--                      particular object categories and can transfer dynamics onto novel unseen object instances. Extensive experiments on diverse objects-->
<!--                      demonstrate the effectiveness of our approach compared to common video prediction frameworks.-->
<!--                      </p>-->
<!--                    </div>-->

<!--                    </div>-->
<!--                </div>-->
            </div>
        </section>

        <!-- Two -->
        <section id="two" class="wrapper style2 special">
            <div class="container">
                <header class="major">
                    <h2>Approach</h2>
                </header>

                <div class="container 80%">
            <div class="image fit captioned align-left"
                        style="margin-bottom:2em; box-shadow:0 0;
                        text-align:justify">
              <img src="paper/teaser.png" alt="" style="border:0px solid black"/>
              During training we learn an behavior representation \(z_\beta\) which is independent of posture by combining a special conditional VAE with an auxiliary decoder \(p_\psi\) measuring the amount of posture present in the behavior sequence. To improve synthesis quality, we relax the latent regularization term in the cVAE-objective and train an invertible transformation \(\mathcal{T}_\xi\) to close the resulting gap between the prior and posterior distributions in a second stage.
              In inference mode, we transfer a given source behavior (green) to an arbitrary target posture (yellow) or synthesize novel behavior from the prior distribution which is matched to q by a learned invertible transformation T (red).
            </div>
                </div>
            </div>
        </section>


        <section id="three" class="wrapper style2 special">
          <div class="container">
            <header class="major">
              <h2>Results</h2>
              <p>and applications of our model.</p>
            </header>


              <header class="minor">
                <h2>Behavior Transfer</h2>
              </header>
              <p>
                   We transfer fine-grained, characteristic body dynamics of an observed behavior \(x_\beta\) to unrelated, significantly different target postures \(x_t\). If required, the target posture is first adjusted by a transition phase before re-enacting the inferred behavior.
              </p>

               <div class="row 150%">
                <div class="6u 12u$(xsmall)">
                    <div class="image fit captioned align-just">
                        <div class="videocontainer">
                            <video controls class="videothing">
                            <source src="images/behavior_transfer1.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                            </video>
                        </div>
                        Visualization of transferred behavior in posture space: Each column shows distinct re-enactements of a given source behavior \(\boldsymbol{x}_\beta\), visualized in the top-row example, for multiple target postures \(x_t\).
                    </div>
                </div>
                <div class="6u$ 12u$(xsmall)">
                    <div class="image fit captioned align-just">
                        <div class="videocontainer">
                            <video controls class="videothing">
                            <source src="images/behavior_transfer2.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                            </video>
                        </div>
                        Visualization of transferred behavior in posture space: Each column shows distinct re-enactements of a given source behavior \(\boldsymbol{x}_\beta\), visualized in the top-row example, for multiple target postures \(x_t\).
                    </div>
                </div>
            </div>
            <div class="row 150%">
                <div class="6u 12u$(xsmall)">
                    <div class="image fit captioned align-just">
                    <div class="videocontainer">
                    <video controls class="videothing">
                    <source src="images/behavior_transfer2_RGB.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                    </video>
                    </div>
                    Visualization of transferred behavior in RGB-space: We generate video sequences by first generating re-enacted posture sequences as explained above. As our proposed disentangling framework can also be applied to posture-appearance transfer (see below for more examples), we can subsequently train a model to disentangle posture from appearance and apply it to generate the individual image frames of a re-enacted posture sequence for a given human appearance. The shown examples visualize this ability based on the synthesized posture sequences depicted above. Note the difference in appearance between the source sequence and the resulting video re-enactments.
                    </div>
                </div>
                <div class="6u$ 12u$(xsmall)">
                     <div class="image fit captioned align-just">
                    <div class="videocontainer">
                    <video controls class="videothing">
                    <source src="images/behavior_transfer1_RGB.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                    </video>
                    </div>
                    Visualization of transferred behavior in RGB-space: We generate video sequences by first generating re-enacted posture sequences as explained above. As our proposed disentangling framework can also be applied to posture-appearance transfer (see below for more examples), we can subsequently train a model to disentangle posture from appearance and apply it to generate the individual image frames of a re-enacted posture sequence for a given human appearance. The shown examples visualize this ability based on the synthesized posture sequences depicted above. Note the difference in appearance between the source sequence and the resulting video re-enactments.
                    </div>
                </div>
            </div>

        <header class="minor">
            <h2>Behavior Sampling</h2>
          </header>
          <p>
               As we learn a parametric prior behavior distribution \(p(z_\beta)\), we can use our model to sample novel, unseen behaviors for a given target posture.
          </p>


        <div class="row 150%">
            <div class="6u 12u$(xsmall)">
                <div class="image fit captioned align-just">
                    <div class="videocontainer">
                        <video controls class="videothing">
                        <source src="images/samples.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                        </video>
                    </div>
                    To visually evaluate the diversity of behavior sequences sampled by our model we visualize six samples drawn from \(z_\beta\) for the same target posture in each row.
                </div>
            </div>
            <div class="6u$ 12u$(xsmall)">

                <div class="image fit captioned align-just">
                <div class="videocontainer">
                <video controls class="videothing">
                <source src="images/nearest_neighbors.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                </div>
                Nearest neighour visualization in behavior and posture space: We re-enact a source behavior \(\boldsymbol{x}_\beta\) using a random target posture \(x_t\). Next, we find its nearest neighbour in the training sequences based on <i>(i)</i> distance between behavior representations \(z_\beta\) and <i>(ii)</i> average distances between postures sequences (based on alignment w.r.t. the pelvis keypoints). Each column depicts a separate example showing the 'Source Behavior', the 'Nearest Neighbor based on Behavior representation', the 'Behavior Re-enactment of Source Behavior' and the 'Nearest Neighbor based on Posture', i.e. average posture distance. We observe that while gthere exist close training sequences in terms of posture, the nearest neighbors based on \(z_\beta\) show <i>similar</i> behavior dynamics while being <i>dissimilar</i> in posture.
                </div>
            </div>
        </div>

        <header class="minor">
            <h2>Behavior Interpolation</h2>
          </header>
          <p>
               We interpolate between the behavior observed in two sequences \(\boldsymbol{x}_\beta^1\) and \(\boldsymbol{x}_\beta^2\). To this end, we first extract their corresponding behavior representations \(z_\beta^1, z_\beta^2\) and interpolate between them at equidistant steps, i.e. \((1 - \lambda) \cdot z_\beta^1 + \lambda \cdot z_\beta^2; \; \lambda \in \{0.0,0.2,0.4,0.6,0.8,1.0\}\). Next, we generate a sequence of interpolated behavior using our decoder \(p_\theta(\boldsymbol{x}|z_\beta,x_t)\) with \(x_t\) being the first frame of \(\boldsymbol{x}_\beta^1\), respectively \(\boldsymbol{x}_\beta^2\). Note, that for \(\lambda \in \{0,1.0\}\) we basically reconstruct the source sequences \(\boldsymbol{x}_\beta^1\), \(\boldsymbol{x}_\beta^2\).
          </p>

        <div class="row 150%">
            <div class="6u 12u$(xsmall)">
                <div class="image fit captioned align-just">
                    <div class="videocontainer">
                        <video controls class="videothing">
                        <source src="images/interpolations_01.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                        </video>
                    </div>
                    Interpolated behavior sequences between two sequences \(\boldsymbol{x}_\beta^1\) and \(\boldsymbol{x}_\beta^2\).
                </div>
            </div>
            <div class="6u$ 12u$(xsmall)">

                <div class="image fit captioned align-just">
                <div class="videocontainer">
                <video controls class="videothing">
                <source src="images/interpolations_rgb_01.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video>
                </div>
                Video sequences synthesized by our model for the interpolated behavior sequences shown on the left.
                </div>
            </div>
        </div>
        <div class="row 150%">
            <div class="6u 12u$(xsmall)">
                <div class="image fit captioned align-just">
                    <div class="videocontainer">
                        <video controls class="videothing">
                        <source src="images/interpolations_02.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                        </video>
                    </div>
                    Interpolated behavior sequences between two sequences \(\boldsymbol{x}_\beta^1\) and \(\boldsymbol{x}_\beta^2\).
                </div>
            </div>
            <div class="6u$ 12u$(xsmall)">

                <div class="image fit captioned align-just">
                <div class="videocontainer">
                <video controls class="videothing">
                <source src="images/interpolations_rgb_02.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video>
                </div>
                Video sequences synthesized by our model for the interpolated behavior sequences shown on the left.
                </div>
            </div>
        </div>
        <div class="row 150%">
            <div class="6u 12u$(xsmall)">
                <div class="image fit captioned align-just">
                    <div class="videocontainer">
                        <video controls class="videothing">
                        <source src="images/interpolations_03.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                        </video>
                    </div>
                    Interpolated behavior sequences between two sequences \(\boldsymbol{x}_\beta^1\) and \(\boldsymbol{x}_\beta^2\).
                </div>
            </div>
            <div class="6u$ 12u$(xsmall)">

                <div class="image fit captioned align-just">
                <div class="videocontainer">
                <video controls class="videothing">
                <source src="images/interpolations_rgb_03.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video>
                </div>
                Video sequences synthesized by our model for the interpolated behavior sequences shown on the left.
                </div>
            </div>
        </div>

    <header class="minor">
            <h2>Posture-Appearance Transfer</h2>
          </header>
          <p>
               Our approach to disentangling latent representations is not only appicable for factorizing static from temporal information, but also to posture-appearance disentangling. We demonstrate this capability of our model on the DeepFashion and Market1501 datasets.
          </p>

    <div class="row 150%">
        <div class="6u 12u$(xsmall)">

            <div class="image fit captioned align-just">
                <a href="images/deepfashion_grid.jpg">
                <img src="images/deepfashion_grid.jpg" alt="" />
                </a>
                Posture-Appearance transfer on DeepFashion. Our method can also be used to disentangle posture and appearance. A quantitative evaluation can be found Table 1 in the supplementary material.
            </div>
            </div>
        <div class="6u$ 12u$(xsmall)">

            <div class="image fit captioned align-just">
                <a href="images/market_grid.jpg">
                <img src="images/market_grid.jpg" alt="" />
                </a>
                Posture-Appearance transfer on Market1501. Our method can also be used to disentangle posture and appearance. A quantitative evaluation can be found Table 1 in the supplementary material.

            </div>
        </div>
    </div>


    </div>
</section>


			<!-- last -->
<section id="four" class="wrapper style3 special"
          style="background-attachment:scroll;background-position:center bottom;">
					<div class="container">
						<header class="major">
							<h2>Acknowledgement</h2>
              <p>
              	The research leading to these results is funded by the German Federal Ministry for Economic Affairs and Energy within the project “KI-Absicherung – Safe AI for automated driving” and by the German Research Foundation (DFG) within project 421703927.
              This page is based on a design by <a href="http://templated.co">TEMPLATED</a>.
              </p>
						</header>
					</div>
				</section>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
